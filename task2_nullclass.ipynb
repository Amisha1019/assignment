{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk0pG16KjoXZWA9f+6+jKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amisha1019/assignment/blob/main/task2_nullclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k9AZ9LJeL2d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
        "        self.key_conv   = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))"
      ],
      "metadata": {
        "id": "4BBSKS5EeUZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "        batch, C, width, height = x.size()\n",
        "\n",
        "        # Query, Key, Value\n",
        "        proj_query = self.query_conv(x).view(batch, -1, width*height).permute(0, 2, 1)  # B x N x C'\n",
        "        proj_key   = self.key_conv(x).view(batch, -1, width*height)                     # B x C' x N\n",
        "        energy     = torch.bmm(proj_query, proj_key)                                    # B x N x N\n",
        "        attention  = F.softmax(energy, dim=-1)                                          # attention map\n",
        "\n",
        "        proj_value = self.value_conv(x).view(batch, -1, width*height)                   # B x C x N\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))                         # B x C x N\n",
        "        out = out.view(batch, C, width, height)\n",
        "\n",
        "        out = self.gamma * out + x   # residual connection\n",
        "        return out"
      ],
      "metadata": {
        "id": "ybaixsCpeWC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, img_dim, text_dim, hidden_dim):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(img_dim, hidden_dim, kernel_size=1)\n",
        "        self.key_linear = nn.Linear(text_dim, hidden_dim)\n",
        "        self.value_linear = nn.Linear(text_dim, hidden_dim)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, img_feat, text_emb):\n",
        "        \"\"\"\n",
        "        img_feat: (B, C, W, H)\n",
        "        text_emb: (B, seq_len, text_dim)\n",
        "        \"\"\"\n",
        "        B, C, W, H = img_feat.size()\n",
        "        seq_len = text_emb.size(1)\n",
        "\n",
        "\n",
        "        queries = self.query_conv(img_feat).view(B, -1, W*H).permute(0, 2, 1)\n",
        "\n",
        "\n",
        "        keys   = self.key_linear(text_emb)\n",
        "        values = self.value_linear(text_emb)\n",
        "\n",
        "        energy = torch.bmm(queries, keys.transpose(1, 2))\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "\n",
        "        out = torch.bmm(attention, values)\n",
        "        out = out.permute(0, 2, 1).contiguous()\n",
        "        out = out.view(B, -1, W, H)\n",
        "\n",
        "        return self.gamma * out + img_feat"
      ],
      "metadata": {
        "id": "cplBH_SsebLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, img_feat, text_emb):\n",
        "        \"\"\"\n",
        "        img_feat: (B, C, W, H)\n",
        "        text_emb: (B, seq_len, text_dim)\n",
        "        \"\"\"\n",
        "        B, C, W, H = img_feat.size()\n",
        "        seq_len = text_emb.size(1)\n",
        "\n",
        "\n",
        "        queries = self.query_conv(img_feat).view(B, -1, W*H).permute(0, 2, 1)\n",
        "\n",
        "\n",
        "        keys   = self.key_linear(text_emb)\n",
        "        values = self.value_linear(text_emb)"
      ],
      "metadata": {
        "id": "LZMk5K5Cef6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energy = torch.bmm(queries, keys.transpose(1, 2))\n",
        "attention = F.softmax(energy, dim=-1)\n",
        "\n",
        "out = torch.bmm(attention, values)\n",
        "out = out.permute(0, 2, 1).contiguous()\n",
        "out = out.view(B, -1, W, H)\n",
        "\n",
        "return self.gamma * out + img_feat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "y45-s0o6ep0n",
        "outputId": "282aee58-411c-4526-e727-a53138bd4584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'queries' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-508583486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'queries' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100, text_dim=256):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Linear(z_dim, 256*8*8)  # project noise\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
        "        self.attn1 = SelfAttention(128)\n",
        "\n",
        "        self.conv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
        "        self.cross_attn = CrossAttention(64, text_dim, hidden_dim=32)\n",
        "\n",
        "        self.conv3 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, z, text_emb):\n",
        "        x = self.fc(z).view(-1, 256, 8, 8)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.attn1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.cross_attn(x, text_emb)\n",
        "\n",
        "        x = self.tanh(self.conv3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "GjykCZrkfTQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, z, text_emb):\n",
        "        x = self.fc(z).view(-1, 256, 8, 8)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.attn1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.cross_attn(x, text_emb)\n",
        "\n",
        "        x = self.tanh(self.conv3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ddz9tXwvfXQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}